# Meridian CI Regression Gate
# Copy this file to your repository's .github/workflows/ directory

name: LLM Eval Regression Check

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  # Optional: scheduled runs
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday

env:
  PYTHON_VERSION: '3.11'

jobs:
  eval-check:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install Meridian
        run: |
          pip install -e .
      
      - name: Run smoke tests (no API keys)
        run: |
          python -m core.cli run \
            --suite math_short \
            --model deepseek_chat \
            --output results/smoke_test.jsonl
      
      - name: Check against baselines
        run: |
          python -m core.cli check \
            --baseline baselines/gpt2_baseline.json \
            --model deepseek_chat
      
      - name: Run unit tests
        run: |
          pytest tests/ -v --tb=short
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: eval-results
          path: results/
          retention-days: 30
      
      - name: Post PR comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## Meridian Results\n\n';
            
            try {
              const results = JSON.parse(fs.readFileSync('results/smoke_test.jsonl'));
              comment += `- Accuracy: ${(results.accuracy * 100).toFixed(1)}%\n`;
              comment += `- Passed: ${results.passed_tests}/${results.total_tests}\n`;
            } catch {
              comment += 'Evaluation completed. See artifacts for details.\n';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Optional: API model evaluation (requires secrets)
  eval-api-models:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: eval-check
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install
        run: pip install -e .
      
      - name: Evaluate GPT-3.5
        if: env.OPENAI_API_KEY != ''
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: |
          python -m core.cli run \
            --suite Meridian_core_50 \
            --model openai_gpt35 \
            --output results/gpt35_eval.jsonl
      
      - name: Upload API results
        uses: actions/upload-artifact@v4
        with:
          name: api-eval-results
          path: results/
          retention-days: 90
